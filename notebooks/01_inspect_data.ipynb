{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757b7a4d-a98b-4fa9-b9a0-59ff68cb6210",
   "metadata": {},
   "source": [
    "# 1. Inspecting your data\n",
    "\n",
    "If you want to solve a problem with machine learning, you'll need three things:\n",
    "\n",
    "1. A good dataset;\n",
    "2. A good model;\n",
    "3. A good optimization algorithm.\n",
    "\n",
    "If any of these three is not good enough, your trained model won't be good either. Many novice machine learning practitioners mainly focus on the second requirement, i.e., the model. While a good model certainly is a vital aspect of a succesful machine learning pipeline, if your data sucks, it doesn't really matter how advanced your model is. **The model can only be as good as the data.**\n",
    "\n",
    "Therefore, your first step in a machine learning project should always be to **inspect your data**. And that's exactly what we'll do now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92294d-cce6-4799-bdab-e1030b604b04",
   "metadata": {},
   "source": [
    "## 1.1 Inspecting the filetree\n",
    "\n",
    "Before you can start inspecting the data itself, you need to know how and where your data is stored.\n",
    "\n",
    "For this example, download the [Gen 1 Pokemon Dataset from Kaggle](https://www.kaggle.com/datasets/echometerhhwl/pokemon-gen-1-38914), extract it and move it to the parent directory of this notebook. Rename the folder from `archive` to `PokemonGen1`.\n",
    "\n",
    "We'll now inspect what's inside the `PokemonGen1` directory. For this, you can use the [`pathlib`](https://docs.python.org/3/library/pathlib.html) module from the Python standard library. A [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path) object is an abstract representation of a path (e.g., to a file or directory) in your operating system.\n",
    "\n",
    "Let's create a `Path` object for the `PokemonGen1` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d518d0-c01e-43f7-a1db-ae1c9a84be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c9030-3bd6-4f87-a8e7-82f1439a3afd",
   "metadata": {},
   "source": [
    "To inspect the files in the directory, we can employ the [`glob()` method of the `Path` class](https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob). This method expects a *pattern* and returns all paths that match the given pattern. In this pattern, an asterisk (`*`) is interpreted as a wildcard. So, to list all files in the `PokemonGen1` directory, we can pass in the pattern `*`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae1cec-b0cb-4386-86e3-951cfbe7360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb99cea-64bb-42da-a9cf-c63a1d1e8892",
   "metadata": {},
   "source": [
    "This will give you something like `<generator object Path.glob at 0x............>`. Indeed, the `glob()` method returns a Python [generator](https://wiki.python.org/moin/Generators), not a list. A generator is an object you can iterate over (just like a list) but that (unlike a list) cannot be indexed. A generator can only tell you the *next* iteration item.\n",
    "\n",
    "> The advantage of generators is that they don't need to store all iteration items up-front. If the iteration items are large, or if there is a large number of items, a generator can save large amounts of memory.\n",
    "\n",
    "To get the next item in a generator, you can use [the built-in Python function `next()`](https://docs.python.org/3/library/functions.html#next):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293077b-6050-460d-b11a-6ddfbdf8a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065be02-2712-4a7c-a961-2e733e2bc72e",
   "metadata": {},
   "source": [
    "Once you have iterated over all items, the generator can be considered *empty* and it will raise a [`StopIteration`](https://docs.python.org/3/library/exceptions.html#StopIteration) exception if you attempt to call `next()` with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdba3ea-0f76-4a1e-a539-0a8fa4f3c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48d83b-b17c-4e8f-99c4-dc8907e9d3c1",
   "metadata": {},
   "source": [
    "The `StopIteration` is already raised with our second `next()` call, so we know that there is only one subdirectory in the `PokemonGen1` directory.\n",
    "\n",
    "Of course, calling `next()` over and over on a generator that contains a lot of items is cumbersome. Luckily, we can also *iterate* over a generator with a simple `for` loop.\n",
    "\n",
    "> ⚠️ **Only use new generators in `for` loops**\n",
    ">\n",
    "> You can think of iterating over a generator with a `for` loop as simply calling `next()` over and over, passing in the generator as an argument and stopping once a `StopIteration` exception is raised. If you have already called `next()` on the generator object, the `for` loop will start wherever the generator left off. **A for loop does not *rewind* a generator before it starts iterating!** (Btw, there is no such thing as *rewinding* a generator. Once an iteration item is returned, the generator forgets about it.) Therefore, to avoid subtle bugs, it is important to only use freshly created generators in `for` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0311ae8-2f7e-4192-8ec0-184c050e68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa543f-2f1e-4736-8aa5-5e929a9fa3b0",
   "metadata": {},
   "source": [
    "We can also collect all iteration items of a generator in a list by passing the generator to the [built-in function](https://docs.python.org/3/library/functions.html) [`list()`](https://docs.python.org/3/library/functions.html#func-list)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e69f40-78fa-4a98-900d-187f1c0d128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e4e55-85ad-43a3-b8ee-a687b946d6d2",
   "metadata": {},
   "source": [
    "...or by employing a [list comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbd1d6-3892-478d-bb21-7758ce820e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deaebe6-cfdb-4e4a-887a-50da6e761284",
   "metadata": {},
   "source": [
    "As we saw from the previous cells, the `PokemonGen1` directory only contains a single directory, i.e., `PokemonGen1/data`. Let's inspect this directory as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cc85e-91cf-4617-83b3-3275a8d79bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78922b66-0943-4707-a043-f8617f7c82dd",
   "metadata": {},
   "source": [
    "We can see that `PokemonGen1/data` contains a large number of subdirectories that correspond to different Pokémon names. How many Pokémon does our dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7e432-3265-4c3f-800c-77dd9a84f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e687c078-c7b6-469e-b630-b74dd94cb01d",
   "metadata": {},
   "source": [
    "We can inspect the total number of files that is in one of the subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b898923-1562-45af-87e0-f66faa510258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ccfbc-ac95-4e43-afa0-6d301957da59",
   "metadata": {},
   "source": [
    "As these file paths are also represented with `pathlib.Path` objects, we can easily extract useful properties of the image paths, like the name, extension, stem, parent path,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b86fc-6264-44b7-b6f6-b15d6ee7c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691be06-2cbf-4e7e-99ae-d51b11242654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2b9cc-3a9d-4192-8c2c-dcb7403d10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7c2a6-f37b-4bd4-8869-a8ba4dfada74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f69acd-f564-4a53-be18-c2b5f376a2a9",
   "metadata": {},
   "source": [
    "The `stem` is the path's name without suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76455a2d-8606-444f-9dde-1e2c8856b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228e235-2225-47e2-9851-58f5846aed06",
   "metadata": {},
   "source": [
    "The `parent` attribute gives the path of the parent directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27feac-c59b-4729-a046-d71371e2a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a270236-ba9a-422c-928d-8c9fc671fc61",
   "metadata": {},
   "source": [
    "Note that we can get the Pokémon's name from the name of the parent directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138ec56-c734-4141-821a-758cb1ec2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a451f1-1d78-4fbc-be38-100a0075bcf2",
   "metadata": {},
   "source": [
    "We can also inspect the file extensions of all these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d9e0d-5c67-464a-873b-fe21fc572012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0528bf-6ba7-4f20-a4dc-0b6874c88bd3",
   "metadata": {},
   "source": [
    "Or, with a set comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52ed80-4139-4ed8-8391-09f0bada0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc1ea3-30f7-49b0-8c3f-4475999db933",
   "metadata": {},
   "source": [
    "From the above cells, we know that the subdirectories contain files with `.jpg` and `.png` extensions. In other words, when globbing the `PokemonGen1/data` directory, we'll get **35 626 images**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216454f-f274-4305-b954-af275ffca5f8",
   "metadata": {},
   "source": [
    "## 1.2 Representing the dataset as a `DataFrame`\n",
    "\n",
    "We're starting to get a feeling of how the files in the dataset are structured. The folder `PokemonGen1` contains a subdirectory `data`, and this directory contains multiple subdirectories, each of which corresponds to a Pokémon. The Pokémon directories contain files that have either a `.jpg` or a `.png` extension (i.e., images).\n",
    "\n",
    "With `pathlib`'s `glob()`, we can explore our dataset in a rudimentary way. To understand our data more deeply, we can represent our dataset as a [`DataFrame` object from the `pandas` library](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). A DataFrame is a tabular data structure consisting of rows and columns, much like an Excel sheet.\n",
    "\n",
    "We want to create a DataFrame that contains two columns: `image` and `label`. The `image` column contains the path to an image of a Pokémon, and the `label` column contains the corresponding name of that Pokémon.\n",
    "\n",
    "You can construct a `DataFrame` by passing in a list of dictionaries. Each dictionary in the list corresponds to a row in the DataFrame. The dictionary keys correspond to column names, and the values to the value that should be in the table cell of that column and row.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269a83f-a62c-4f05-bfa3-b10dcce3b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054c339-2b9f-44d4-b8a9-05add1e91b48",
   "metadata": {},
   "source": [
    "We can iterate over all available data and store the image paths along with the Pokémon names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67a7d3-aa53-44d8-9afa-4bd73db071c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256dc22-a26a-4b4e-8177-d60ab624b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c871aa-4406-48f2-a49b-ca260d6b1d18",
   "metadata": {},
   "source": [
    "Of course, you can also create the same DataFrame with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d167c0-a5c4-46cb-980f-38d28b9badcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84002af6-3e48-42d6-8e09-6e5dcfe9e2fb",
   "metadata": {},
   "source": [
    "## 1.3 Visualizing data imbalance\n",
    "\n",
    "A common problem in machine learning is *data imbalance*. This means that some classes have much more examples than others. Such an imbalance could cause a difference in model performance on the majority classes vs. the minority classes.\n",
    "\n",
    "To visualize the number of images per class, we can use the plotting library [Matplotlib](https://matplotlib.org/).\n",
    "\n",
    "In Matplotlib, plots are drawn on [`Figure`s](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure). A `Figure` contains one or more [`Axes`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.html#matplotlib.axes.Axes), which is an area that will contain the actual plot.\n",
    "\n",
    "To create a `Figure` with a single `Axes`, you can call [`matplotlib.pyplot.subplots()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots) without any arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c9670b-1914-4ee3-abec-0347fd671a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73a03d-6fa4-4ddd-9fb6-e02f567fbf3b",
   "metadata": {},
   "source": [
    "Now, we'll use the [`DataFrame.groupby()` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) to group the rows by `label` and apply [the built-in `len()` function](https://docs.python.org/3/library/functions.html#len) on each group to get the number of images per label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aea882-6907-4b85-a16f-d81247d5b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caae808-e281-4603-a3b5-6f5173b1ed5b",
   "metadata": {},
   "source": [
    "Let's put this in a variable called `count_per_label`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5353e-e42e-49c8-9094-d386ca8e5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be1df4-ba1d-41ff-b6a2-64cc4db7722d",
   "metadata": {},
   "source": [
    "Now we want to draw a bar chart in our `Axes` with the labels as x-values and the counts as bar height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bd16b-902c-4331-89ff-d23a9117dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e59f6e-6746-4495-8827-21c3a67a4174",
   "metadata": {},
   "source": [
    "Woops, that looks pretty cluttered. Let's try with a larger figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf7e32-5028-4383-86de-5e215ba46c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf84259-76c8-4339-9816-ea9a269ae339",
   "metadata": {},
   "source": [
    "The bars are much clearer now, but the tick labels are still overlapping... We can *rotate* the tick labels with [`Axes.tick_params()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.tick_params.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb404c-ba52-4321-937c-cb7ceae04fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0fafc-088a-4909-aa23-ea874db13e85",
   "metadata": {},
   "source": [
    "That's much more readable! To get a real sense of the data imbalance, it's best to **sort the labels in descending order of size**. You can do this by calling [`sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.sort_values.html) on `count_per_label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f4f5c-464b-4079-ac4b-e91d083dbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc608e6-95f5-4e05-9b62-ef771252b9cb",
   "metadata": {},
   "source": [
    "Let's see what we get now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31a6bf-245b-46da-af6a-020737f99a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef71e65-689a-4a5a-80e5-01272cb0574c",
   "metadata": {},
   "source": [
    "Let's put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720580e-1b91-4efd-9f08-cd60b83d5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ed404-e251-4ee9-8d4b-8c596d6159b9",
   "metadata": {},
   "source": [
    "Now, you should be careful drawing too many conclusions from this graph alone. It is not necessarily true that our model won't work on minority classes. However, the data imbalance we observe is something to take into account when evaluating a trained model, and it might push us toward data sampling strategies to counter the imbalance, if necessary.\n",
    "\n",
    "To get an even better intuition of our data, let's take a look at some images!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46289445-f03f-4c57-acaf-3d648d7cb6b2",
   "metadata": {},
   "source": [
    "## 1.4 Inspecting images and labels\n",
    "### 1.4.1 Visualizing a single image\n",
    "\n",
    "Let's visualize the first image in our dataset.\n",
    "\n",
    "To obtain the elements of a column in a DataFrame, you can pass in the column name between square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad87d50-3af1-49b6-85bb-06b62cd2a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab80c1-fb56-46f9-8338-d80e4cc2bb7e",
   "metadata": {},
   "source": [
    "The returned object is a pandas [`Series`](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) object. This is somewhat comparable to a list. To get the value of the `image` column at row `0`, you can index with `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ff73e-d78d-403b-839a-8e6e9d5caa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e77938-0b13-428d-ba7e-74aceadd98bb",
   "metadata": {},
   "source": [
    "Now, we can use [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open) to open the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f990619-e038-40c2-9a0e-65f1fe679f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebce3d-a388-4b55-8980-9b127622d433",
   "metadata": {},
   "source": [
    "As you can see, our image is represented as a PyTorch tensor of shape $\\text{Channels}\\times\\text{Height}\\times\\text{Width}$, or simply $\\text{C}\\times\\text{H}\\times\\text{W}$. To visualize this tensor, we can also use Matplotlib.\n",
    "\n",
    "Let's create another `Figure` with a single `Axes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4375f-49d4-430e-979c-b32cf0ebf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21903ee3-9b76-4445-8e13-f11da7c85d64",
   "metadata": {},
   "source": [
    "Now, we can draw our image on the `Axes` with [`Axes.imshow()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.imshow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f79c0-b10f-4105-87a5-50be75af3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447ee79-fb79-4e79-a4e4-591c1cd84b52",
   "metadata": {},
   "source": [
    "To remove the ticks and tick labels, we can use [`Axes.tick_params()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.tick_params.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d9463-fa9d-4ce1-a242-b2b9947b990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4c74d-3215-433b-8dd6-2aae6768b8c1",
   "metadata": {},
   "source": [
    "And to remove the contouring black lines, we make the [*spines* of the `Axes`](https://matplotlib.org/stable/api/spines_api.html) invisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed0ee5-b206-47bc-919d-330a617b227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937022b1-4430-452f-a7ff-1833648520ae",
   "metadata": {},
   "source": [
    "Putting it all together, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6e864-059e-4cc1-a1e8-9e241440c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e731ae5-65f3-4922-8103-34c080310ff5",
   "metadata": {},
   "source": [
    "### 1.4.2 Visualizing multiple images\n",
    "\n",
    "Instead of viewing images one by one, we can save some time by visualizing a **grid** of images. To get the first 10 image paths in the DataFrame, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cc3cb-2655-4b97-82fd-224477d81cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecc2ec-8ad5-415b-8c89-777107a8a3e7",
   "metadata": {},
   "source": [
    "To visualize the image paths in this `Series` object, we'll need **multiple `Axes`** in our `Figure`. You can pass a number of rows (`nrows`) and a number of columns (`ncols`) to [`matplotlib.pyplot.subplots()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots).\n",
    "\n",
    "For example, to create a `Figure` with 10 `Axes` in a 2 x 5 grid, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c1e6c-89b0-42da-a35e-1f410ee796c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2da63-5f27-420a-91c5-5c53cc6e2e8d",
   "metadata": {},
   "source": [
    "The returned `axes` is a $2\\times 5$ [numpy array](https://numpy.org/doc/stable/reference/generated/numpy.array.html) of `Axes` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915352d1-4e8d-408c-ac82-d99cbcad9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3e82e-5126-4c5a-9d0e-7fbf99776012",
   "metadata": {},
   "source": [
    "We now want to call `imshow()` on each of these `Axes` objects, each time with another image we read in with `Image.open()`. To avoid nasty index computations, we can simply flatten our `axes` array, to get a flat numpy array of $10$ `Axes` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0663a2-c894-43ba-b1cd-d11061482bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330e978-434b-4bee-9279-5361a845157a",
   "metadata": {},
   "source": [
    "Now, we can iterate jointly over these 10 `Axes` objects and the first 10 image paths in our DataFrame. Such a joint iteration is easily done with [Python's built-in function `zip()`](https://docs.python.org/3/library/functions.html#zip).\n",
    "\n",
    "In the loop itself, we can reuse our code to visualize a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbf675-ba18-4915-ae84-33182385c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e629038-b55c-4dbe-9773-5e493b736e18",
   "metadata": {},
   "source": [
    "You can call [`Figure.tight_layout()`](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.tight_layout) to reduce the padding between the `Axes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430bcd7-4fae-426c-a405-04e0f2f80468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e82fc7-cd7a-4008-b73a-8d3990bf74d6",
   "metadata": {},
   "source": [
    "We can also draw random samples from our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56772a7a-11f4-45cf-ab28-cb860cf36b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400637a7-89e2-4e92-bf9a-ffa0bbaaeecb",
   "metadata": {},
   "source": [
    "Let's take the `image` column from such a sampled DataFrame and visualize those images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9200149-a38e-4d23-8fcb-4f6527575b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd5f9a-daba-4681-bcad-4c7f4b74fd30",
   "metadata": {},
   "source": [
    "### 1.4.3 Adding labels\n",
    "\n",
    "To make it more insightful, we can put the corresponding label on top of each image. To access the label of an image in the iteration, we'll need to iterate over entire rows of the DataFrame, as each row contains information on both the image and the corresponding label. To iterate over the rows of a DataFrame, you can use [`DataFrame.iterrows()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cf9e6-b9e5-4ba2-96ae-7bb706a27686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e554ee9-dbd7-4b1f-b8cf-45a3831a2677",
   "metadata": {},
   "source": [
    "We can now access both the image path and the label of each row in the sampled DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6e175-70b8-4755-a692-c3df536c5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81b827-b94a-4b49-9ef2-63c0fea20806",
   "metadata": {},
   "source": [
    "Now, to put the Pokémon's name on top of each image, we can use [`Axes.set_title()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_title.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a8773-1fcb-4831-a1dc-4cee5f6542ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5e2cd-7c1e-45d8-ab3a-e84a69738a2f",
   "metadata": {},
   "source": [
    "### 1.4.4 Visualizing images with a particular label\n",
    "\n",
    "Another interesting inspection is to visualize some random images of a **particular label**. To get all DataFrame rows that belong to a certain label, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424f1b1d-6707-4639-8a6c-27e63ad1a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4f802-9901-41b2-a25e-867bed447bad",
   "metadata": {},
   "source": [
    "Now, we can just sample from these rows and run the same code as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee588ed1-5ec9-4c8d-9c3a-81dc9db466dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572eb2bf-3580-4efc-96e6-9af84c33d653",
   "metadata": {},
   "source": [
    "## 1.5 Inspecting data transforms\n",
    "\n",
    "### 1.5.1 Transform images to the same size\n",
    "\n",
    "When visualizing the images in the dataset, you might have noticed that the images have all kinds of sizes. When training a neural network, we'll want to create *batches* of images. Such a batch can only be created from images of the **same size**. Before we can start training, thus, we'll need a way to give each image the same size.\n",
    "\n",
    "The [`torchvision` library](https://pytorch.org/vision/stable/index.html) contains some handy tools to help us with that. More specifically, we can make use of the transforms in [`torchvision.transforms.v2`](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html). For example, with [`v2.Resize`](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize), we can resize an image to a fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f9c5e-3b67-4a72-bdab-aa65cb045733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a868100-409a-40a2-893e-9440a1528849",
   "metadata": {},
   "source": [
    "To apply this resize transform to an image, we simply call the transform with the image as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3aa47-bb51-4c2b-9a96-d574f5ba3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad327fe-30b7-4a2f-b794-16bd49337fa0",
   "metadata": {},
   "source": [
    "As you can see, `Resize()` has resized our image such that the smallest size became equal to `224`. You can also pass in a width and height to `Resize()`, but this might change the aspect ratio of your image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c17ce-90e4-4d07-87f5-2352559ed353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b8342-be0d-40be-8105-d162c6630095",
   "metadata": {},
   "source": [
    "What if we want all images to be of size $224\\times 224$, but don't want our aspect ratio to drastically change? Well, we can resize the shortest side of the image to a fixed size and then **crop** out the center square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4919562-7a0c-4a55-9954-ccfa6d032438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c773b-2b70-4389-9454-5c9e7e74fe1e",
   "metadata": {},
   "source": [
    "An easier way to apply a chain of transforms to an image is with [`v2.Compose`](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768fc19-966e-4e1b-8760-5110e2619947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726cce9e-03d1-4799-9a2a-f6045157dc51",
   "metadata": {},
   "source": [
    "We can also apply this transform to multiple images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dca63-7814-48ae-a07e-9cf0da7d797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e851-1b40-4625-b26d-c23959dc6c5d",
   "metadata": {},
   "source": [
    "### 1.5.2 Data augmentation\n",
    "\n",
    "Apart from ensuring that all images can be put in a batch, another important use of data transforms is to **augment the dataset**. The idea of *data augmentation* is to apply image transforms that our model should be invariant against. If a certain image is somewhat zoomed in, the aspect ratio has changed slightly, or the image is rotated by 10°, for example, we still want our model to recognize the Pokémon.\n",
    "\n",
    "Let's first put our image visualization logic inside a utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfb3a1-8a96-49d3-8fae-211dbce98c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875de9b5-8cc9-4cde-807e-0d28fad2e6dd",
   "metadata": {},
   "source": [
    "Next, we sample some images from the DataFrame and show them without any transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405205c-131b-4c70-b02e-5c35c09d16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deec166-83fd-4c46-a427-dbe2ae4bb62a",
   "metadata": {},
   "source": [
    "Let's now play around with some combinations of transforms and visualize their effect on the images. See [this page](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended) for an overview of the available image transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6eeae-ade2-4a35-9925-b43914de0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b459c-8bc2-4967-863e-a27ea8c9083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c6dc7-f9b1-4f39-9516-6ea305e628a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f3cc1-b21c-4531-8dd2-e7fe6a203bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9e18f-d3bc-470d-933c-c16ab28854fc",
   "metadata": {},
   "source": [
    "### 1.5.3 Data type conversion and normalization\n",
    "\n",
    "Our images are [`PIL.Image` instances](https://pillow.readthedocs.io/en/stable/reference/Image.html). Our neural network, however, will expect `torch.Tensor` objects with `torch.float32` numbers that are (roughly) normally distributed with zero mean and unit variance.\n",
    "\n",
    "To convert a `PIL.Image` to a PyTorch tensor, you can use [`v2.ToImage()`](https://pytorch.org/vision/main/generated/torchvision.transforms.v2.ToImage.html). The name of this transform might be somewhat confusing at first sight, but it is named so because it converts the input into a [`torchvision.tv_tensors.Image`](https://pytorch.org/vision/main/generated/torchvision.tv_tensors.Image.html#torchvision.tv_tensors.Image) instance, which is a subclass of [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6c7bd-0183-4105-a39c-2959300699c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeec010-6860-4714-9704-6e91a549034f",
   "metadata": {},
   "source": [
    "As you can see, the data stored in our `PIL.Image` consists of 8 bit unsigned integers (0 - 255). To convert this to `torch.float32` numbers, we can use the [`v2.ToDtype()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.ToDtype.html#torchvision.transforms.v2.ToDtype) transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6dcc8-e9e9-4cb5-b541-b13612f54973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085c5ac-e208-416c-81cc-5449c88a17eb",
   "metadata": {},
   "source": [
    "But as you can see, these numbers are still between 0 and 255. If we pass `scale=True`, the numbers will be scaled between `0.0` and `1.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8bb3f-4e30-4337-a3d3-5f17f9c409ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7a439-4562-4590-8153-f1106b7ad921",
   "metadata": {},
   "source": [
    "Now, with [`v2.Normalize()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.Normalize.html#torchvision.transforms.v2.Normalize), we can normalize our pixel values so that they'll approximately have zero mean and unit variance. `Normalize` expects two arguments: `mean` (the mean to subtract) and `std` (the standard deviation to divide by). We will use `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. These values are the mean and std of [the ImageNet dataset](https://www.image-net.org/index.php), which is often used to pretrain neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad45c2a-d583-45f0-a2ce-7cea070acddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd76878-a796-4568-8384-6fb2d7b60d00",
   "metadata": {},
   "source": [
    "We can again chain these transforms with `v2.Compose()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b43930-79c8-4b7d-8387-18e4329e243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ac4e2-9cdb-4bb9-b40f-efc52576fb15",
   "metadata": {},
   "source": [
    "And we can prepend them with other transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d073dc4-cd0e-4b46-a0eb-b50988983a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097faae-e463-4f34-a438-ccf0c314b6cf",
   "metadata": {},
   "source": [
    "For some transforms, you might gain a small amount of processing time by moving `v2.ToImage()` to the beginning of the transforms list, as the subsequent transforms will then be applied directly to PyTorch tensors instead of PIL images. Therefore, it is customary to use `v2.ToImage()` as the first transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94551de4-c3cf-4a01-9334-c3892be6f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172af240-7976-4e1e-a8cb-923e9f9e411d",
   "metadata": {},
   "source": [
    "Note that these normalized tensors cannot really be visualized directly, as the data range extends beyond $[0.0, 1.0]$ and the pixels will need to be clipped. These values won't be clipped for the neural network, however, so a visualization will not really be representative of what the neural network will actually receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2550ac-b6f1-4b62-9705-fafe403c398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... WRITE YOUR CODE HERE ... #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}